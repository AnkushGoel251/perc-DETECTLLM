{"cells":[{"cell_type":"markdown","metadata":{"id":"Eprvb8JtpX0d"},"source":["# Final DataSet Creation\n","-------------------"]},{"cell_type":"markdown","metadata":{"id":"r8ssdKmwq1pZ"},"source":["## Planning :\n","\n","   1. First read out all available data/parquet files\n","   2. Using `Gemini` manipulate the data\n","      \n","      2.1  Select one of the lines from Human Generated data and rewrite using `Gemini`, that's how we are going to create combination of llm generated text and human generated text.\n","\n","      2.2 Using the defined function, we will be counting no of words generated by `LLM(Gemini)` and already we will be having count of words available from `Human`.\n","\n","      2.3 A new column will be storing percentage of `LLM` generated text, using the formula:\n","          \n","          llm_generated_perc = word count for llm generated data / total word count in the text\n","\n","      2.4 Make sure to iterate the same fuction for different number of lines.\n","      Example : First we will be regenerating one of lines from human generated text, next 2 and next 3 lines and so on. This will help to enhance the data model.\n","\n","   3. Finally summing up all functions to create the final dataset.\n","\n","   \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nLbDLNXjpms9"},"source":["### 1. Loading the dataset\n","--------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_1CCHSMpp2a"},"outputs":[],"source":["%%capture\n","!pip install fastparquet\n","!pip install langchain_google_genai\n","!pip install langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdQVk_fBp1Jc"},"outputs":[],"source":["## importing necessary libraries\n","\n","import pandas as pd\n","import numpy as np\n","\n","from tqdm import tqdm\n","\n","## importing libraries for generative ai functionality\n","from langchain_google_genai import GoogleGenerativeAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.prompts import PromptTemplate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMPDTXw1WeGB"},"outputs":[],"source":["## setting up the environment\n","\n","from google.colab import userdata\n","GENAI_API_KEY = userdata.get(\"GENAI_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5800,"status":"ok","timestamp":1712435340235,"user":{"displayName":"SAIBAL PATRA","userId":"12093096904720386748"},"user_tz":-330},"id":"22Cc9E4hpAik","outputId":"7f1604ea-b808-4646-f37d-187c721ee85e"},"outputs":[],"source":["## loading the data\n","\n","detectllm_data = pd.read_parquet(\"/content/drive/MyDrive/BTP 8th SEM/Data/Dataset/DetectLLM.parquet\", engine = 'fastparquet')\n","detectllm_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-FLRN6fY6LT"},"outputs":[],"source":["## take a look at the data\n","\n","detectllm_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFVIJL7-BAWt"},"outputs":[],"source":["## take a look at the data\n","\n","detectllm_data.tail()"]},{"cell_type":"markdown","metadata":{"id":"GkefVjrdZUfb"},"source":["#### 1.1 Seperating Human Written Data only\n","-------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9uimVEsZTQJ"},"outputs":[],"source":["## human data\n","detectllm_data_human = detectllm_data[detectllm_data['Label'] == 'Human']\n","\n","## finally checking the output data\n","# detectllm_data_human.shape\n","detectllm_data_human.Label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"Q23LxsiwqOib"},"source":["### 2. Data Manipulation\n","---------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP-LEw8eqKfR"},"outputs":[],"source":["'''\n","   1. setting up a function, sentence_splitter , that will split the Text Data on the appearance of full stop and a list containing all the splitted sentences will appear\n","   2. Next there will a function called regenerate_sentence, it will take all these list as an in input and regenerate using any llms.\n","   3. The trick here is for every list, it will tweak every possible element and then store the tweaked elements in a new column, regenerated_text\n","'''\n","\n","\n","'''\n","   Initiating the model and initiating the chain\n","   using GoogleGenerativeAI and using chain\n","'''\n","\n","## defining safety_settings to ignore the errors\n","# safety_settings = [\n","#   {\n","#   \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n","#   \"threshold\": \"BLOCK_NONE\"\n","#   },\n","#   {\n","#   \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n","#   \"threshold\": \"BLOCK_NONE\"\n","#   },\n","#   {\n","#   \"category\": \"HARM_CATEGORY_HARASSMENT\",\n","#   \"threshold\": \"BLOCK_NONE\"\n","#   },\n","#   {\n","#   \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n","#   \"threshold\": \"BLOCK_NONE\"\n","#    },\n","# ]\n","\n","safety_settings = {\n","    \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": {\"threshold\": \"BLOCK_NONE\"},\n","    \"HARM_CATEGORY_HATE_SPEECH\": {\"threshold\": \"BLOCK_NONE\"},\n","    \"HARM_CATEGORY_HARASSMENT\": {\"threshold\": \"BLOCK_NONE\"},\n","    \"HARM_CATEGORY_DANGEROUS_CONTENT\": {\"threshold\": \"BLOCK_NONE\"}\n","}\n","\n","model = GoogleGenerativeAI(model='models/gemini-pro',\n","                           google_api_key=GENAI_API_KEY)\n","\n","## initializing the chain and the parser for better result\n","parser = StrOutputParser()\n","\n","\n","## Using an Prompt Template\n","template = \"\"\"\n","        Rewrite the sentence, no constains in no of lines.\n","        Sentence : {sentence}\n","\"\"\"\n","## creating prompt\n","prompt = PromptTemplate.from_template(template)\n","\n","chain = prompt | model | parser  ##chain will be used for further text generation\n","\n","\n","# Step 1: Splitting the text column into sentences and storing the result in a new column\n","def sentence_splitter(text):\n","    \"\"\"\n","    Splits the input text into sentences based on full stops.\n","    Args:\n","        text (str): Input text containing sentences separated by full stops.\n","    Returns:\n","        list: List of individual sentences.\n","    \"\"\"\n","    text = text.replace('[', '').replace(']', '').replace('\"', \"\")\n","    sentences = text.split('. ')\n","    return sentences\n","\n","def split_text_column(df):\n","    \"\"\"\n","    Splits the 'Text' column into sentences and stores the result in a new column 'splitted_text'.\n","    Args:\n","        df (DataFrame): DataFrame containing 'Text' column with text data.\n","    Returns:\n","        DataFrame: DataFrame with an additional column 'splitted_text'.\n","    \"\"\"\n","    df['splitted_text'] = df['Text'].apply(sentence_splitter)\n","    return df\n","\n","# Step 2: Regenerate sentences using an LLMs model\n","def regenerate_sentence(sentences):\n","    \"\"\"\n","    Regenerates sentences using a generative AI model.\n","    Args:\n","        sentences (list): List of sentences.\n","    Returns:\n","        list: List of regenerated sentences.\n","    \"\"\"\n","    regenerated_sentences = []\n","    for sentence in sentences:\n","        if isinstance(sentence, str):\n","            generated_sentence = chain.invoke(sentence)\n","            regenerated_sentences.append(generated_sentence)\n","    return regenerated_sentences\n","\n","\n","def regenerate_sentences_in_dataframe(df):\n","    \"\"\"\n","    Regenerates each splitted sentence using an LLMs model and stores the regenerated\n","    sentences in a new column 'regenerated_text'.\n","    Args:\n","        df (DataFrame): DataFrame containing 'splitted_text' column with lists of sentences.\n","    Returns:\n","        DataFrame: DataFrame with an additional column 'regenerated_text'.\n","    \"\"\"\n","    df['regenerated_text'] = df['splitted_text'].apply(regenerate_sentence)\n","    return df\n","\n","## Step 3 : Regenerate the whole Text\n","def regenerate_text(text):\n","    \"\"\"\n","    Regenerates the entire text using a generative AI model.\n","    Args:\n","        text (str): The input text to be regenerated.\n","    Returns:\n","        str: The regenerated text.\n","    \"\"\"\n","    text = text.replace('[', '').replace(']', '').replace('\"', \"\")\n","    regenerated_text = chain.invoke([text])\n","    return regenerated_text\n","\n","def regenerate_text_in_dataframe(df):\n","    \"\"\"\n","    Regenerates the entire text in the 'Text' column of the DataFrame using an LLMs model.\n","    Args:\n","        df (DataFrame): DataFrame containing 'Text' column with text data.\n","    Returns:\n","        DataFrame: DataFrame with an additional column 'regenerated_text'.\n","    \"\"\"\n","    df.loc[:, 'regenerated_text'] = df['char_less_text'].apply(regenerate_text)\n","    return df\n","\n","\n","# Step 4: Remove Extra character\n","def char_remover(text):\n","    \"\"\"\n","    Splits the input text into sentences based on full stops.\n","    Args:\n","        text (str): Input text containing sentences separated by full stops.\n","    Returns:\n","        list: List of individual sentences.\n","    \"\"\"\n","    text = text.replace(\"['\", '').replace(\"']\", '').replace('\"', \"\").replace(\"'\", \"\")\n","    return text\n","\n","def char_remover_column(df):\n","    \"\"\"\n","    Splits the 'Text' column into sentences and stores the result in a new column 'splitted_text'.\n","    Args:\n","        df (DataFrame): DataFrame containing 'Text' column with text data.\n","    Returns:\n","        DataFrame: DataFrame with an additional column 'splitted_text'.\n","    \"\"\"\n","    df['char_less_text'] = df['Text'].apply(char_remover)\n","    return df\n","\n","def regenerate_text_in_dataframe(df):\n","    \"\"\"\n","    Regenerates the entire text in the 'Text' column of the DataFrame using an LLMs model.\n","    Args:\n","        df (DataFrame): DataFrame containing 'Text' column with text data.\n","    Returns:\n","        DataFrame: DataFrame with an additional column 'regenerated_text'.\n","    \"\"\"\n","    df['regenerated_text'] = df['char_less_text'].apply(lambda x: regenerate_text(x))\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4a2pTSlqeQN"},"outputs":[],"source":["# detectllm_data['Text'][0].split(\".\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yonnGVMFrDiu"},"outputs":[],"source":["# sentence_splitter(detectllm_data['Text'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHOcRr-oxUjR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_8pG6eEzr_l"},"outputs":[],"source":["## applying the functions [split_text_column] to split the sentence\n","\n","# split_text_column(df=detectllm_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAPW1tsQUzdT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DylnCnVTZxhu"},"source":["### 3. Generating New Data [Applying the function]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Bbgs65iQZtd"},"outputs":[],"source":["detectllm_data_human.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QsPhJF_Zw6j"},"outputs":[],"source":["## Removal of Extra character is done first\n","splitted_data = char_remover_column(detectllm_data_human)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pRfSwe7VQUk"},"outputs":[],"source":["## testing splitted data\n","splitted_data.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhGF17LJOzL8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BHJ-ViXOzJF"},"outputs":[],"source":["## applying the second function\n","# regenerated_data = regenerate_text_in_dataframe(splitted_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcDQ5iF4OzGc"},"outputs":[],"source":["# regenerated_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ipj3YPWQSXUq"},"outputs":[],"source":["regenerate_text(splitted_data['char_less_text'][0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnrqR5M_CJbR"},"outputs":[],"source":["## adding new columns using `assign` method in dataframe\n","\n","splitted_data.assign(regenerated_text = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSqbFcfKTEmy"},"outputs":[],"source":["## generating new data\n","\n","\n","regenerate_text_in_dataframe(splitted_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNV8P4L89-Rn"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM4yFhgprNU7EXCmrP2XALf","mount_file_id":"1bvZp4ybVz45Dqo71523PSQxWheEl2KaA","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
